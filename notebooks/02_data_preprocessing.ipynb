{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a66fe7c",
   "metadata": {},
   "source": [
    "### Data Cleaning: Handling Missing Values\n",
    "\n",
    "**Decision:** Remove all rows containing missing values.\n",
    "\n",
    "**Justification:**\n",
    "* **Low Impact:** The rows with missing data represent approximately **2.6%** of the total dataset. Removing this small fraction will not statistically bias the results.\n",
    "* **Critical Spatial Data:** The majority of missing values (6,974 entries) are in the `Latitude`, `Longitude`, and `Location` columns. Since the `Block` addresses are redacted for privacy (e.g., `015XX`), accurate imputation of exact coordinates is impossible. Without these coordinates, the data is unusable for spatial analysis.\n",
    "* **Data Integrity:** Removing these rows ensures a 100% complete dataset, preventing errors in downstream machine learning models (e.g., Logistic Regression) that cannot handle `NaN` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0888e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                         0\n",
      "Case Number                0\n",
      "Date                       0\n",
      "Block                      0\n",
      "IUCR                       0\n",
      "Primary Type               0\n",
      "Description                0\n",
      "Location Description     613\n",
      "Arrest                     0\n",
      "Domestic                   0\n",
      "Beat                       0\n",
      "District                   0\n",
      "Ward                       2\n",
      "Community Area            13\n",
      "FBI Code                   0\n",
      "X Coordinate            6974\n",
      "Y Coordinate            6974\n",
      "Year                       0\n",
      "Updated On                 0\n",
      "Latitude                6974\n",
      "Longitude               6974\n",
      "Location                6974\n",
      "dtype: int64\n",
      "================================================================================\n",
      "ID                      0.000000\n",
      "Case Number             0.000000\n",
      "Date                    0.000000\n",
      "Block                   0.000000\n",
      "IUCR                    0.000000\n",
      "Primary Type            0.000000\n",
      "Description             0.000000\n",
      "Location Description    0.231424\n",
      "Arrest                  0.000000\n",
      "Domestic                0.000000\n",
      "Beat                    0.000000\n",
      "District                0.000000\n",
      "Ward                    0.000755\n",
      "Community Area          0.004908\n",
      "FBI Code                0.000000\n",
      "X Coordinate            2.632870\n",
      "Y Coordinate            2.632870\n",
      "Year                    0.000000\n",
      "Updated On              0.000000\n",
      "Latitude                2.632870\n",
      "Longitude               2.632870\n",
      "Location                2.632870\n",
      "dtype: float64\n",
      "================================================================================\n",
      "Missing values after dropping:\n",
      "0\n",
      "================================================================================\n",
      "New data shape: (257752, 22)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "df = pd.read_csv('../data/raw_data.csv')\n",
    "\n",
    "# Count of missing values per column\n",
    "print(df.isnull().sum())\n",
    "print(\"==\" *40)\n",
    "# Percentage of missing values per column\n",
    "print((df.isnull().sum() / len(df)) * 100)\n",
    "print(\"==\" *40)\n",
    "# Drop all rows that have at least one missing value\n",
    "df_clean = df.dropna()\n",
    "\n",
    "# Verify that the data is now clean\n",
    "print(\"Missing values after dropping:\")\n",
    "print(df_clean.isnull().sum().sum())\n",
    "print(\"==\" *40)\n",
    "# Check how many rows are left\n",
    "print(f\"New data shape: {df_clean.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d4819b",
   "metadata": {},
   "source": [
    "### Fix Data Types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "989a957f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types fixed.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 257752 entries, 1 to 264880\n",
      "Data columns (total 22 columns):\n",
      " #   Column                Non-Null Count   Dtype         \n",
      "---  ------                --------------   -----         \n",
      " 0   ID                    257752 non-null  int64         \n",
      " 1   Case Number           257752 non-null  object        \n",
      " 2   Date                  257752 non-null  datetime64[ns]\n",
      " 3   Block                 257752 non-null  object        \n",
      " 4   IUCR                  257752 non-null  object        \n",
      " 5   Primary Type          257752 non-null  object        \n",
      " 6   Description           257752 non-null  object        \n",
      " 7   Location Description  257752 non-null  object        \n",
      " 8   Arrest                257752 non-null  bool          \n",
      " 9   Domestic              257752 non-null  bool          \n",
      " 10  Beat                  257752 non-null  int64         \n",
      " 11  District              257752 non-null  int64         \n",
      " 12  Ward                  257752 non-null  float64       \n",
      " 13  Community Area        257752 non-null  float64       \n",
      " 14  FBI Code              257752 non-null  object        \n",
      " 15  X Coordinate          257752 non-null  float64       \n",
      " 16  Y Coordinate          257752 non-null  float64       \n",
      " 17  Year                  257752 non-null  int64         \n",
      " 18  Updated On            257752 non-null  object        \n",
      " 19  Latitude              257752 non-null  float64       \n",
      " 20  Longitude             257752 non-null  float64       \n",
      " 21  Location              257752 non-null  object        \n",
      "dtypes: bool(2), datetime64[ns](1), float64(6), int64(4), object(9)\n",
      "memory usage: 41.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# Convert 'Date' to datetime objects\n",
    "df_clean = df_clean.copy()\n",
    "df_clean['Date'] = pd.to_datetime(df_clean['Date'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "# Ensure categorical columns are strings \n",
    "categorical_cols = ['Primary Type', 'Description', 'Location Description']\n",
    "for col in categorical_cols:\n",
    "    df_clean[col] = df_clean[col].astype(str)\n",
    "\n",
    "print(\"Data types fixed.\")\n",
    "df_clean.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45c1eba",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29f7454a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering Complete. New columns added:\n",
      "                 Date  Hour  Day_of_Week  Is_Weekend  Season\n",
      "1 2015-12-31 23:59:00    23            3           0  Winter\n",
      "2 2015-12-31 23:55:00    23            3           0  Winter\n",
      "3 2015-12-31 23:50:00    23            3           0  Winter\n",
      "4 2015-12-31 23:50:00    23            3           0  Winter\n",
      "5 2015-12-31 23:45:00    23            3           0  Winter\n"
     ]
    }
   ],
   "source": [
    "# Extract Basic Time Components\n",
    "df_clean['Hour'] = df_clean['Date'].dt.hour\n",
    "df_clean['Day_of_Week'] = df_clean['Date'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "df_clean['Month'] = df_clean['Date'].dt.month\n",
    "\n",
    "# Identify Weekends (1 if Sat/Sun, else 0)\n",
    "# Day 5 is Saturday, Day 6 is Sunday\n",
    "df_clean['Is_Weekend'] = df_clean['Day_of_Week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# Map Months to Seasons\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Fall'\n",
    "\n",
    "df_clean['Season'] = df_clean['Month'].apply(get_season)\n",
    "\n",
    "print(\"Feature Engineering Complete. New columns added:\")\n",
    "print(df_clean[['Date', 'Hour', 'Day_of_Week', 'Is_Weekend', 'Season']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ebd94a",
   "metadata": {},
   "source": [
    "### Outlier Detection using IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "421df940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers removed. Shape changed from (257752, 27) to (256553, 27)\n"
     ]
    }
   ],
   "source": [
    "def remove_outliers_iqr(df, column):\n",
    "    # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Define bounds (1.5 times the IQR is the standard rule)\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Filter data to keep only values within bounds\n",
    "    df_filtered = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "    return df_filtered\n",
    "\n",
    "# Apply to Latitude and Longitude to remove GPS errors\n",
    "original_shape = df_clean.shape\n",
    "df_clean = remove_outliers_iqr(df_clean, 'Latitude')\n",
    "df_clean = remove_outliers_iqr(df_clean, 'Longitude')\n",
    "\n",
    "print(f\"Outliers removed. Shape changed from {original_shape} to {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6ca463",
   "metadata": {},
   "source": [
    "### Feature Engineering: Aggregating Crime Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c25179ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated features created. Preview:\n",
      "   Community Area  Hour  Area_Hour_Crime_Count  Season  \\\n",
      "0            68.0    23                    351  Winter   \n",
      "1            45.0    23                     53  Winter   \n",
      "2             2.0    23                    131  Winter   \n",
      "3             6.0    23                    255  Winter   \n",
      "4            25.0    23                    767  Winter   \n",
      "\n",
      "   Area_Season_Crime_Count  \n",
      "0                     1416  \n",
      "1                      303  \n",
      "2                      728  \n",
      "3                     1084  \n",
      "4                     3708  \n"
     ]
    }
   ],
   "source": [
    "# Strategy: Calculate \"Crime Intensity\" for each Community Area at each Hour.\n",
    "# This answers: \"How dangerous is this specific area at this specific time of day usually?\"\n",
    "\n",
    "# 1. Group by Area and Hour to count crimes\n",
    "crime_intensity = df_clean.groupby(['Community Area', 'Hour']).size().reset_index(name='Area_Hour_Crime_Count')\n",
    "\n",
    "# 2. Merge this count back into the main dataframe\n",
    "# 'how=left' ensures we keep all original rows and just add the new info\n",
    "df_clean = df_clean.merge(crime_intensity, on=['Community Area', 'Hour'], how='left')\n",
    "\n",
    "# 3. Create a broader \"Seasonal Risk\" by Area and Season\n",
    "seasonal_risk = df_clean.groupby(['Community Area', 'Season']).size().reset_index(name='Area_Season_Crime_Count')\n",
    "df_clean = df_clean.merge(seasonal_risk, on=['Community Area', 'Season'], how='left')\n",
    "\n",
    "print(\"Aggregated features created. Preview:\")\n",
    "print(df_clean[['Community Area', 'Hour', 'Area_Hour_Crime_Count', 'Season', 'Area_Season_Crime_Count']].head())\n",
    "\n",
    "# import os\n",
    "\n",
    "# # Create the folder if it doesn't exist\n",
    "# os.makedirs('../data', exist_ok=True)\n",
    "\n",
    "# # Save the master clean file\n",
    "# master_output_path = '../data/crime_data_clean.csv'\n",
    "# df_clean.to_csv(master_output_path, index=False)\n",
    "# print(f\"Master dataset saved to {master_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53dfb34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ID Case Number                 Date                     Block  IUCR  \\\n",
      "0  10365064    HZ100370  2015-12-31 23:59:00       075XX S EMERALD AVE  1320   \n",
      "1  10364662    HZ100006  2015-12-31 23:55:00  079XX S STONY ISLAND AVE  0430   \n",
      "2  10364740    HZ100010  2015-12-31 23:50:00         024XX W FARGO AVE  0820   \n",
      "3  10364683    HZ100002  2015-12-31 23:50:00          037XX N CLARK ST  0460   \n",
      "4  10366580    HZ102701  2015-12-31 23:45:00        050XX W CONCORD PL  1310   \n",
      "\n",
      "      Primary Type                    Description Location Description  \\\n",
      "0  CRIMINAL DAMAGE                     TO VEHICLE               STREET   \n",
      "1          BATTERY  AGGRAVATED: OTHER DANG WEAPON               STREET   \n",
      "2            THEFT                 $500 AND UNDER            APARTMENT   \n",
      "3          BATTERY                         SIMPLE             SIDEWALK   \n",
      "4  CRIMINAL DAMAGE                    TO PROPERTY            APARTMENT   \n",
      "\n",
      "   Arrest  Domestic  ...   Latitude  Longitude  \\\n",
      "0   False     False  ...  41.757367 -87.642993   \n",
      "1   False     False  ...  41.751270 -87.585822   \n",
      "2   False     False  ...  42.016804 -87.690709   \n",
      "3    True     False  ...  41.949837 -87.658635   \n",
      "4   False     False  ...  41.910470 -87.751597   \n",
      "\n",
      "                             Location  Hour Day_of_Week  Month  Is_Weekend  \\\n",
      "0  POINT (-87.642992854 41.757366519)    23           3     12           0   \n",
      "1  POINT (-87.585822373 41.751270452)    23           3     12           0   \n",
      "2  POINT (-87.690708662 42.016804165)    23           3     12           0   \n",
      "3  POINT (-87.658635101 41.949837364)    23           3     12           0   \n",
      "4  POINT (-87.751597381 41.910469677)    23           3     12           0   \n",
      "\n",
      "   Season Area_Hour_Crime_Count  Area_Season_Crime_Count  \n",
      "0  Winter                   351                     1416  \n",
      "1  Winter                    53                      303  \n",
      "2  Winter                   131                      728  \n",
      "3  Winter                   255                     1084  \n",
      "4  Winter                   767                     3708  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "df1= pd.read_csv('../data/crime_data_clean.csv')\n",
    "\n",
    "print(df1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2c2512",
   "metadata": {},
   "source": [
    "### Step 1: Define Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba015dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the base list of relevant columns\n",
    "# We drop 'ID', 'Case Number', 'Date', 'Block', 'IUCR', 'Description', 'Location'\n",
    "# We Keep: Categorical context, Time info, Coordinates, and your new Aggregated counts\n",
    "selected_features = [\n",
    "    'Arrest',                  # Target\n",
    "    'Primary Type',            # Feature (Categorical)\n",
    "    'Location Description',    # Feature (Categorical)\n",
    "    'Domestic',                # Feature (Boolean)\n",
    "    'Hour',                    # Feature (Num)\n",
    "    'Day_of_Week',             # Feature (Num)\n",
    "    'Month',                   # Feature (Num)\n",
    "    'Is_Weekend',              # Feature (Num)\n",
    "    'Season',                  # Feature (Categorical)\n",
    "    'Latitude',                # Feature (Num)\n",
    "    'Longitude',               # Feature (Num)\n",
    "    'Area_Hour_Crime_Count',   # Feature (Num - Aggregated)\n",
    "    'Area_Season_Crime_Count'  # Feature (Num - Aggregated)\n",
    "]\n",
    "\n",
    "# Create a subset dataframe\n",
    "df_model_base = df_clean[selected_features].copy()\n",
    "\n",
    "# Fix Boolean to Int (True/False -> 1/0)\n",
    "df_model_base['Arrest'] = df_model_base['Arrest'].astype(int)\n",
    "df_model_base['Domestic'] = df_model_base['Domestic'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf82c374",
   "metadata": {},
   "source": [
    "### Step 2: Create Dataframe for Tree Models (Random Forest, LGBM, XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4d88039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree-based data saved (Label Encoded).\n",
      "   Arrest  Primary Type  Location Description  Domestic  Hour  Day_of_Week  \\\n",
      "0       0             6                   118         0    23            3   \n",
      "1       0             2                   118         0    23            3   \n",
      "2       0            30                    17         0    23            3   \n",
      "3       1             2                   114         0    23            3   \n",
      "4       0             6                    17         0    23            3   \n",
      "\n",
      "   Month  Is_Weekend  Season   Latitude  Longitude  Area_Hour_Crime_Count  \\\n",
      "0     12           0       3  41.757367 -87.642993                    351   \n",
      "1     12           0       3  41.751270 -87.585822                     53   \n",
      "2     12           0       3  42.016804 -87.690709                    131   \n",
      "3     12           0       3  41.949837 -87.658635                    255   \n",
      "4     12           0       3  41.910470 -87.751597                    767   \n",
      "\n",
      "   Area_Season_Crime_Count  \n",
      "0                     1416  \n",
      "1                      303  \n",
      "2                      728  \n",
      "3                     1084  \n",
      "4                     3708  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a copy for Tree models\n",
    "df_tree = df_model_base.copy()\n",
    "\n",
    "# Initialize Encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Encode the string columns\n",
    "categorical_cols = ['Primary Type', 'Location Description', 'Season']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    df_tree[col] = le.fit_transform(df_tree[col])\n",
    "\n",
    "# Save for Random Forest / XGB / LGBM\n",
    "df_tree.to_csv('../data/data_for_tree_models.csv', index=False)\n",
    "print(\"Tree-based data saved (Label Encoded).\")\n",
    "print(df_tree.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d96c99",
   "metadata": {},
   "source": [
    "### Step 3: Create Dataframe for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a90f0134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression data saved (One-Hot Encoded).\n",
      "   Arrest  Domestic  Hour  Day_of_Week  Month  Is_Weekend   Latitude  \\\n",
      "0       0         0    23            3     12           0  41.757367   \n",
      "1       0         0    23            3     12           0  41.751270   \n",
      "2       0         0    23            3     12           0  42.016804   \n",
      "3       1         0    23            3     12           0  41.949837   \n",
      "4       0         0    23            3     12           0  41.910470   \n",
      "\n",
      "   Longitude  Area_Hour_Crime_Count  Area_Season_Crime_Count  ...  \\\n",
      "0 -87.642993                    351                     1416  ...   \n",
      "1 -87.585822                     53                      303  ...   \n",
      "2 -87.690709                    131                      728  ...   \n",
      "3 -87.658635                    255                     1084  ...   \n",
      "4 -87.751597                    767                     3708  ...   \n",
      "\n",
      "   Location Description_VEHICLE - DELIVERY TRUCK  \\\n",
      "0                                          False   \n",
      "1                                          False   \n",
      "2                                          False   \n",
      "3                                          False   \n",
      "4                                          False   \n",
      "\n",
      "   Location Description_VEHICLE - OTHER RIDE SERVICE  \\\n",
      "0                                              False   \n",
      "1                                              False   \n",
      "2                                              False   \n",
      "3                                              False   \n",
      "4                                              False   \n",
      "\n",
      "   Location Description_VEHICLE - OTHER RIDE SHARE SERVICE (E.G., UBER, LYFT)  \\\n",
      "0                                              False                            \n",
      "1                                              False                            \n",
      "2                                              False                            \n",
      "3                                              False                            \n",
      "4                                              False                            \n",
      "\n",
      "   Location Description_VEHICLE NON-COMMERCIAL  \\\n",
      "0                                        False   \n",
      "1                                        False   \n",
      "2                                        False   \n",
      "3                                        False   \n",
      "4                                        False   \n",
      "\n",
      "   Location Description_VEHICLE-COMMERCIAL  Location Description_WAREHOUSE  \\\n",
      "0                                    False                           False   \n",
      "1                                    False                           False   \n",
      "2                                    False                           False   \n",
      "3                                    False                           False   \n",
      "4                                    False                           False   \n",
      "\n",
      "   Location Description_YARD  Season_Spring  Season_Summer  Season_Winter  \n",
      "0                      False          False          False           True  \n",
      "1                      False          False          False           True  \n",
      "2                      False          False          False           True  \n",
      "3                      False          False          False           True  \n",
      "4                      False          False          False           True  \n",
      "\n",
      "[5 rows x 176 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create a copy for Linear models\n",
    "df_log = df_model_base.copy()\n",
    "\n",
    "# Apply One-Hot Encoding (get_dummies)\n",
    "# drop_first=True prevents multicollinearity (e.g., if it's not Spring, Summer, or Fall, it MUST be Winter)\n",
    "df_log = pd.get_dummies(df_log, columns=['Primary Type', 'Location Description', 'Season'], drop_first=True)\n",
    "\n",
    "# Note: Logistic Regression usually requires SCALING (StandardScaler).\n",
    "# We usually do scaling INSIDE the training loop to avoid data leakage, \n",
    "# but the dataframe structure is ready now.\n",
    "\n",
    "# Save for Logistic Regression\n",
    "df_log.to_csv('../data/data_for_logistic.csv', index=False)\n",
    "print(\"Logistic Regression data saved (One-Hot Encoded).\")\n",
    "print(df_log.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
